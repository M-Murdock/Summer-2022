{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some rough preliminary code for checking English -> Greek translations. As of right now, it can only identify individual Greek words which are obviously incorrect. Word lists are pulled from Pharr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio [documentation](https://gradio.app/docs/)\n",
    "\n",
    "greek-accentuation [documentation](https://github.com/jtauber/greek-accentuation/blob/master/docs.rst)\n",
    "\n",
    "greek-normalization [documentation](https://github.com/jtauber/greek-normalisation/blob/master/tests.rst)\n",
    "\n",
    "(I'm also using a couple files from [greek-inflexion](https://github.com/jtauber/greek-inflexion/blob/master/README.md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: typing-extensions in /opt/anaconda3/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.8/site-packages (3.0.13)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (3.8.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (3.3.2)\n",
      "Requirement already satisfied: python-multipart in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (0.0.5)\n",
      "Requirement already satisfied: orjson in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (3.7.2)\n",
      "Requirement already satisfied: paramiko in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (2.11.0)\n",
      "Requirement already satisfied: pandas in /Users/mallard/.local/lib/python3.8/site-packages (from gradio) (1.4.2)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: Jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (2.11.2)\n",
      "Requirement already satisfied: analytics-python in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: pycryptodome in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (3.14.1)\n",
      "Requirement already satisfied: markdown-it-py[linkify,plugins] in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (2.1.0)\n",
      "Requirement already satisfied: uvicorn in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (0.17.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (2.24.0)\n",
      "Requirement already satisfied: fastapi in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (0.78.0)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (8.0.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from gradio) (1.19.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->gradio) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->gradio) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->gradio) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->gradio) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->gradio) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->gradio) (2.0.12)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->gradio) (2.8.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from python-multipart->gradio) (1.15.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /opt/anaconda3/lib/python3.8/site-packages (from paramiko->gradio) (3.2.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from paramiko->gradio) (3.1.1)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from paramiko->gradio) (1.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->gradio) (2020.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from Jinja2->gradio) (1.1.1)\n",
      "Requirement already satisfied: backoff==1.10.0 in /opt/anaconda3/lib/python3.8/site-packages (from analytics-python->gradio) (1.10.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from analytics-python->gradio) (1.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.8/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.1)\n",
      "Requirement already satisfied: linkify-it-py~=1.0; extra == \"linkify\" in /opt/anaconda3/lib/python3.8/site-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
      "Requirement already satisfied: mdit-py-plugins; extra == \"plugins\" in /opt/anaconda3/lib/python3.8/site-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.0)\n",
      "Requirement already satisfied: asgiref>=3.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from uvicorn->gradio) (3.5.2)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/anaconda3/lib/python3.8/site-packages (from uvicorn->gradio) (0.13.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.8/site-packages (from uvicorn->gradio) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->gradio) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->gradio) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->gradio) (1.22)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/anaconda3/lib/python3.8/site-packages (from fastapi->gradio) (1.8.2)\n",
      "Requirement already satisfied: starlette==0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from fastapi->gradio) (0.19.1)\n",
      "Requirement already satisfied: cffi>=1.1 in /opt/anaconda3/lib/python3.8/site-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.14.3)\n",
      "Requirement already satisfied: uc-micro-py in /opt/anaconda3/lib/python3.8/site-packages (from linkify-it-py~=1.0; extra == \"linkify\"->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi->gradio) (4.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from starlette==0.19.1->fastapi->gradio) (3.6.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.20)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi->gradio) (1.2.0)\n",
      "Requirement already satisfied: greek-accentuation==1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: greek-normalisation in /opt/anaconda3/lib/python3.8/site-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install typing-extensions --upgrade\n",
    "!pip install gradio\n",
    "!pip install greek-accentuation==1.2.0\n",
    "!pip install greek-normalisation\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import gradio as gr\n",
    "from greek_accentuation.syllabify import *\n",
    "from greek_normalisation.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put the treebank data into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read in `paradigms.tsv` and `verbs.tsv` (from [here](https://github.com/jtauber/greek-inflexion/tree/master/homer-data)) as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-dab219acfad7>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df1 = pd.read_csv(paradigms, sep=r' +\t*', on_bad_lines='skip', header=0, names=['Lemma', 'Type', 'Inflected'])\n"
     ]
    }
   ],
   "source": [
    "# paradigms.tsv contains all forms from Pharr\n",
    "paradigms = \"lib/paradigms.tsv\"\n",
    "# verbs.tsv contains verbs from Pharr\n",
    "verbs = \"lib/verbs.tsv\"\n",
    "\n",
    "# convert to dataframes\n",
    "df1 = pd.read_csv(paradigms, sep=r' +\t*', on_bad_lines='skip', header=0, names=['Lemma', 'Type', 'Inflected'])\n",
    "df2 = pd.read_csv(verbs, sep='\t', on_bad_lines='skip', header=0, names=['Lemma', 'Type', 'Inflected'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read in `tbankplus.txt` from [here](https://raw.githubusercontent.com/gregorycrane/Homerica/master/tlg0012-tbankplus.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-883d359df122>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df3 = pd.read_csv(tbank, sep=r'\\t', on_bad_lines='skip', header=0, names=['col1', 'col2', 'col3', 'Lemma', 'Inflected', 'col6', 'col7', 'Type', 'col8'])\n"
     ]
    }
   ],
   "source": [
    "# paradigms.tsv contains all forms from Pharr\n",
    "tbank = \"lib/tlg0012-tbankplus.txt\"\n",
    "\n",
    "# convert to dataframe\n",
    "df3 = pd.read_csv(tbank, sep=r'\\t', on_bad_lines='skip', header=0, names=['col1', 'col2', 'col3', 'Lemma', 'Inflected', 'col6', 'col7', 'Type', 'col8'])\n",
    "df3 = df3[['Lemma', 'Type', 'Inflected']]\n",
    "# get a dataframe of the inflected forms\n",
    "inflected_df3 = df3.loc[:, 'Inflected']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Lemma     Type  Inflected\n",
      "0       ἀείδω  PRED_CO      ἄειδε\n",
      "1         θεά      ExD        θεά\n",
      "2   Πηληιάδης      ATR  Πηληϊάδεω\n",
      "3    Ἀχιλλεύς      ATR    Ἀχιλῆος\n",
      "4   οὐλόμενος      ATR  οὐλομένην\n",
      "5          ὅς      SBJ          ἣ\n",
      "6      μυρίος      ATR      μυρί’\n",
      "7      Ἀχαιός      OBJ    Ἀχαιοῖς\n",
      "8       ἄλγος      OBJ      ἄλγε’\n",
      "9      τίθημι   ATR_CO      ἔθηκε\n",
      "10      πολύς      ATR     πολλάς\n",
      "11         δέ     AuxY         δ’\n",
      "12    ἴφθιμος      ATR   ἰφθίμους\n",
      "13    ἴφθιμος      ATR   ἰφθίμους\n",
      "14       ψυχή      OBJ      ψυχάς\n",
      "15      Ἅιδης      OBJ       Ἄϊδι\n",
      "16   προιάπτω   ATR_CO   προΐαψεν\n",
      "17   προιάπτω   ATR_CO   προΐαψεν\n",
      "18       ἥρως      ATR      ἡρώων\n",
      "19       ἥρως      ATR      ἡρώων\n",
      "20      αὐτός      OBJ     αὐτούς\n",
      "21         δέ    COORD         δέ\n",
      "22    ἑλώριον    OCOMP     ἑλώρια\n",
      "23      τεύχω   ATR_CO      τεῦχε\n",
      "24       κύων   OBJ_CO   κύνεσσιν\n",
      "25     οἰωνός   OBJ_CO   οἰωνοῖσί\n",
      "26         τε    COORD         τε\n",
      "27        πᾶς      ATR       πᾶσι\n",
      "28       Ζεύς      ATR       Διός\n",
      "29         δέ    COORD         δ’\n"
     ]
    }
   ],
   "source": [
    "print(df3.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- merge the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes\n",
    "df = pd.concat([df1, df2, df3])\n",
    "\n",
    "# get a list of all the inflected forms\n",
    "inflected_forms = df.loc[:, 'Inflected'].tolist()\n",
    "\n",
    "# get a list of the lemmas\n",
    "lemmas = df.loc[:, 'Lemma'].tolist()\n",
    "\n",
    "# get a list of all the inflected forms without accents\n",
    "inflected_no_accents = [strip_accents(str(element)) for element in inflected_forms]\n",
    "# print(inflected_no_accents[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent = []\n",
    "key_sent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean and format the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove extraneous spaces, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the cleaned input\n",
    "def clean(input):\n",
    "    # remove punctuation\n",
    "    input = ''.join(letter for letter in input if letter not in string.punctuation)\n",
    "    # remove extraneous whitespace\n",
    "    input = ' '.join(input.split())\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the answer key and user answer into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts strings to lists, returns nothing\n",
    "def listify(key, input):\n",
    "    global key_sent \n",
    "    key_sent = key.split(\" \")\n",
    "    global input_sent \n",
    "    input_sent = input.split(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check breathing marks and accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a string of feedback, corrects any errors so that we can proceed\n",
    "def check_breathing_accents(input):\n",
    "    global key_sent\n",
    "    global input_sent\n",
    "    \n",
    "    feedback = ''\n",
    "    for index, word in enumerate(input_sent):\n",
    "        \n",
    "        # check breathing marks\n",
    "        correct = add_necessary_breathing(word)\n",
    "        if correct != word:\n",
    "            feedback += word + ' does not contain the correct breathing marks \\n'\n",
    "            input_sent[index] = correct\n",
    "            word = correct\n",
    "\n",
    "        # check accents\n",
    "        if not word in key_sent:\n",
    "            for key_word in key_sent:\n",
    "                stripped = strip_accents(word)\n",
    "                key_stripped = strip_accents(key_word)\n",
    "                if stripped == key_stripped:\n",
    "                    feedback += word + ' does not contain the correct accents \\n'\n",
    "                    input_sent[index] = key_word\n",
    "            \n",
    "        else:\n",
    "            feedback += word + ' is a valid word \\n'\n",
    "            \n",
    "    return feedback\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check sentence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares the number of words in the key and user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len():\n",
    "    global key_sent\n",
    "    global input_sent\n",
    "    if len(key_sent) > len(input_sent):\n",
    "        return 'Your sentence may be missing one or more words\\n'\n",
    "    elif len(key_sent) < len(input_sent):\n",
    "        return 'Your sentence may have one or more extraneous words\\n'\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check whether the tenses/numbers match the answer key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Iliad treebank](https://github.com/gregorycrane/Homerica/blob/master/tlg0012-tbankplus.txt) to check whether the given word is correct but in an incorrect form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word inputted:\n",
    "# 1. checks whether the form matches any word in the key precisely (if so, move to the next word)\n",
    "# 2. converts word to lemma, compares against the lemmas of each word in the key (if ther is a match, notify the user)\n",
    "# 3. compares the word's lemma (with no accents) to the lemmas of each word in the key (with no accents)\n",
    "# 4. otherwise, notify the user that the word is invalid\n",
    "\n",
    "def check_tense_number():\n",
    "    feedback = ''\n",
    "    global key_sent \n",
    "    global input_sent\n",
    "\n",
    "    # get the lemmas of all the words in the key\n",
    "    key_lemmas = []\n",
    "    for word in key_sent:\n",
    "        index = inflected_forms.index(word) if word in inflected_forms else None\n",
    "        if index != None:\n",
    "            # get the lemma\n",
    "            key_lemmas.append(lemmas[index])\n",
    "        else:\n",
    "            key_lemmas.append('')\n",
    "    \n",
    "  \n",
    "    for word in input_sent:\n",
    "        # if the word isn't in the answer key\n",
    "        if not word in key_sent:\n",
    "            # get the lemma index\n",
    "            index = inflected_forms.index(word) if word in inflected_forms else None\n",
    "            # if there is a lemma for the given word\n",
    "            if index != None:\n",
    "                lem = lemmas[index]\n",
    "                if lem in key_lemmas:\n",
    "                    feedback += (word + ' is the correct word, but not the correct form \\n')\n",
    "                else:\n",
    "                    feedback += (word + ' is a valid Greek word, but is not correct in this translation \\n')\n",
    "            else:\n",
    "                feedback += (word + ' could not be found \\n')\n",
    "\n",
    "    \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we call all the important functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(key, input):\n",
    "    feedback = ''\n",
    "    \n",
    "    input = clean(input)\n",
    "    key = clean(key)\n",
    "\n",
    "    listify(key, input)\n",
    "    feedback += check_breathing_accents(input)\n",
    "    feedback += check_len()\n",
    "    feedback += check_tense_number()\n",
    "    \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a .txt file of questions as input. Each line of the file represents one question. It should contain the English translation of the sentence, followed by a colon, followed by the Greek sentence.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the file name here:\n",
    "quiz = 'lib/quiz_questions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to hold each line of the file\n",
    "lines = []\n",
    "# list of dictionaries for holding the English answer/ Greek answer\n",
    "exercises = []\n",
    "\n",
    "# Read in the lines from the file\n",
    "with open(quiz) as f:\n",
    "    # create list for holding the exercises\n",
    "    lines = f.readlines()\n",
    "\n",
    "# For each line, use regex to grab the answer and full sentence\n",
    "for sent in lines:\n",
    "    \n",
    "    # Get the greek answer\n",
    "    eng_ans_end = sent.find(':')\n",
    "    english_answer = sent[0:eng_ans_end]\n",
    "\n",
    "    greek_answer = sent[eng_ans_end+1:]\n",
    "    \n",
    "    # Add everything to our list of dictionaries\n",
    "    exercises.append({\"english answer\":english_answer, \"greek answer\":greek_answer})\n",
    "\n",
    "# this is just for testing purposes\n",
    "# for i in exercises:\n",
    "#     print(i)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7927/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7927/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7fa9fd735bb0>, 'http://127.0.0.1:7927/', None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback None(<Task finishe...> result=None>)\n",
      "handle: <Handle>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "TypeError: 'NoneType' object is not callable\n"
     ]
    }
   ],
   "source": [
    "exercise_interfaces = []\n",
    "index = 0\n",
    "for ex in exercises:\n",
    "    # Get the Greek sentence\n",
    "    greek_answer = exercises[index][\"greek answer\"]\n",
    "\n",
    "    # Get the English sentence\n",
    "    english_answer = exercises[index][\"english answer\"]\n",
    "\n",
    "    \n",
    "    desc = \"Translate the following sentence into Greek: \" \n",
    "    \n",
    "    exercise_interfaces.append(gr.Interface(fn=get_feedback, description=desc,\n",
    "                    inputs=[gr.Textbox(lines=1, value=greek_answer, visible=False), gr.Textbox(lines=2, placeholder=\"Enter Greek translation here...\", label=english_answer)],\n",
    "                    outputs=\"text\"))\n",
    "    index += 1\n",
    "\n",
    "# name each of the exercise tabs\n",
    "tab_names = list(range(len(exercises)))\n",
    "tab_names = [('Ex.'+str(x+1)) for x in tab_names]\n",
    "\n",
    "# Launch the interface\n",
    "user_interface = gr.TabbedInterface(exercise_interfaces, tab_names)\n",
    "user_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
