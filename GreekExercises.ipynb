{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some rough preliminary code for checking English -> Greek translations. As of right now, it can only identify individual Greek words which are obviously incorrect. Word lists are pulled from Pharr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio [documentation](https://gradio.app/docs/)\n",
    "\n",
    "greek-accentuation [documentation](https://github.com/jtauber/greek-accentuation/blob/master/docs.rst)\n",
    "\n",
    "greek-normalization [documentation](https://github.com/jtauber/greek-normalisation/blob/master/tests.rst)\n",
    "\n",
    "(I'm also using a couple files from [greek-inflexion](https://github.com/jtauber/greek-inflexion/blob/master/README.md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install typing-extensions --upgrade\n",
    "!pip install gradio\n",
    "!pip install greek-accentuation==1.2.0\n",
    "!pip install greek-normalisation\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import gradio as gr\n",
    "from greek_accentuation.syllabify import *\n",
    "from greek_normalisation.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put the treebank data into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read in `paradigms.tsv` and `verbs.tsv` (from [here](https://github.com/jtauber/greek-inflexion/tree/master/homer-data)) as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paradigms.tsv contains all forms from Pharr\n",
    "paradigms = \"lib/paradigms.tsv\"\n",
    "# verbs.tsv contains verbs from Pharr\n",
    "verbs = \"lib/verbs.tsv\"\n",
    "\n",
    "# convert to dataframes\n",
    "df1 = pd.read_csv(paradigms, sep=r' +\t*', on_bad_lines='skip', header=0, names=['Lemma', 'Type', 'Inflected'])\n",
    "df2 = pd.read_csv(verbs, sep='\t', on_bad_lines='skip', header=0, names=['Lemma', 'Type', 'Inflected'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read in `tbankplus.txt` from [here](https://raw.githubusercontent.com/gregorycrane/Homerica/master/tlg0012-tbankplus.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paradigms.tsv contains all forms from Pharr\n",
    "tbank = \"lib/tlg0012-tbankplus.txt\"\n",
    "\n",
    "# convert to dataframe\n",
    "df3 = pd.read_csv(tbank, sep=r'\\t', on_bad_lines='skip', header=0, names=['col1', 'col2', 'col3', 'Lemma', 'Inflected', 'col6', 'col7', 'Type', 'col8'])\n",
    "df3 = df3[['Lemma', 'Type', 'Inflected']]\n",
    "# get a dataframe of the inflected forms\n",
    "inflected_df3 = df3.loc[:, 'Inflected']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- merge the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes\n",
    "df = pd.concat([df1, df2, df3])\n",
    "\n",
    "# get a list of all the inflected forms\n",
    "inflected_forms = df.loc[:, 'Inflected'].tolist()\n",
    "\n",
    "# get a list of the lemmas\n",
    "lemmas = df.loc[:, 'Lemma'].tolist()\n",
    "\n",
    "# get a list of all the inflected forms without accents\n",
    "inflected_no_accents = [strip_accents(str(element)) for element in inflected_forms]\n",
    "# print(inflected_no_accents[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent = []\n",
    "key_sent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean and format the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove extraneous spaces, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the cleaned input\n",
    "def clean(input):\n",
    "    # remove punctuation\n",
    "    input = ''.join(letter for letter in input if letter not in string.punctuation)\n",
    "    # remove extraneous whitespace\n",
    "    input = ' '.join(input.split())\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the answer key and user answer into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts strings to lists, returns nothing\n",
    "def listify(key, input):\n",
    "    global key_sent \n",
    "    key_sent = key.split(\" \")\n",
    "    global input_sent \n",
    "    input_sent = input.split(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check breathing marks and accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a string of feedback, corrects any errors so that we can proceed\n",
    "def check_breathing_accents(input):\n",
    "    global key_sent\n",
    "    global input_sent\n",
    "    \n",
    "    feedback = ''\n",
    "    for index, word in enumerate(input_sent):\n",
    "        \n",
    "        # check breathing marks\n",
    "        correct = add_necessary_breathing(word)\n",
    "        if correct != word:\n",
    "            feedback += word + ' does not contain the correct breathing marks \\n'\n",
    "            input_sent[index] = correct\n",
    "            word = correct\n",
    "\n",
    "        # check accents\n",
    "        if not word in key_sent:\n",
    "            for key_word in key_sent:\n",
    "                stripped = strip_accents(word)\n",
    "                key_stripped = strip_accents(key_word)\n",
    "                if stripped == key_stripped:\n",
    "                    feedback += word + ' does not contain the correct accents \\n'\n",
    "                    input_sent[index] = key_word\n",
    "            \n",
    "        else:\n",
    "            feedback += word + ' is a valid word \\n'\n",
    "            \n",
    "    return feedback\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check sentence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares the number of words in the key and user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len():\n",
    "    global key_sent\n",
    "    global input_sent\n",
    "    if len(key_sent) > len(input_sent):\n",
    "        return 'Your sentence may be missing one or more words\\n'\n",
    "    elif len(key_sent) < len(input_sent):\n",
    "        return 'Your sentence may have one or more extraneous words\\n'\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check whether the tenses/numbers match the answer key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Iliad treebank](https://github.com/gregorycrane/Homerica/blob/master/tlg0012-tbankplus.txt) to check whether the given word is correct but in an incorrect form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word inputted:\n",
    "# 1. checks whether the form matches any word in the key precisely (if so, move to the next word)\n",
    "# 2. converts word to lemma, compares against the lemmas of each word in the key (if ther is a match, notify the user)\n",
    "# 3. compares the word's lemma (with no accents) to the lemmas of each word in the key (with no accents)\n",
    "# 4. otherwise, notify the user that the word is invalid\n",
    "\n",
    "def check_tense_number():\n",
    "    feedback = ''\n",
    "    global key_sent \n",
    "    global input_sent\n",
    "\n",
    "    # get the lemmas of all the words in the key\n",
    "    key_lemmas = []\n",
    "    for word in key_sent:\n",
    "        index = inflected_forms.index(word) if word in inflected_forms else None\n",
    "        if index != None:\n",
    "            # get the lemma\n",
    "            key_lemmas.append(lemmas[index])\n",
    "        else:\n",
    "            key_lemmas.append('')\n",
    "    \n",
    "  \n",
    "    for word in input_sent:\n",
    "        # if the word isn't in the answer key\n",
    "        if not word in key_sent:\n",
    "            # get the lemma index\n",
    "            index = inflected_forms.index(word) if word in inflected_forms else None\n",
    "            # if there is a lemma for the given word\n",
    "            if index != None:\n",
    "                lem = lemmas[index]\n",
    "                if lem in key_lemmas:\n",
    "                    feedback += (word + ' is the correct word, but not the correct form \\n')\n",
    "                else:\n",
    "                    feedback += (word + ' is a valid Greek word, but is not correct in this translation \\n')\n",
    "            else:\n",
    "                feedback += (word + ' could not be found \\n')\n",
    "\n",
    "    \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we call all the important functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(key, input):\n",
    "    feedback = ''\n",
    "    \n",
    "    input = clean(input)\n",
    "    key = clean(key)\n",
    "\n",
    "    listify(key, input)\n",
    "    feedback += check_breathing_accents(input)\n",
    "    feedback += check_len()\n",
    "    feedback += check_tense_number()\n",
    "    \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a .txt file of questions as input. Each line of the file represents one question. It should contain the English translation of the sentence, followed by a colon, followed by the Greek sentence.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the file name here:\n",
    "quiz = 'lib/quiz_questions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to hold each line of the file\n",
    "lines = []\n",
    "# list of dictionaries for holding the English answer/ Greek answer\n",
    "exercises = []\n",
    "\n",
    "# Read in the lines from the file\n",
    "with open(quiz) as f:\n",
    "    # create list for holding the exercises\n",
    "    lines = f.readlines()\n",
    "\n",
    "# For each line, use regex to grab the answer and full sentence\n",
    "for sent in lines:\n",
    "    \n",
    "    # Get the greek answer\n",
    "    eng_ans_end = sent.find(':')\n",
    "    english_answer = sent[0:eng_ans_end]\n",
    "\n",
    "    greek_answer = sent[eng_ans_end+1:]\n",
    "    \n",
    "    # Add everything to our list of dictionaries\n",
    "    exercises.append({\"english answer\":english_answer, \"greek answer\":greek_answer})\n",
    "\n",
    "# this is just for testing purposes\n",
    "# for i in exercises:\n",
    "#     print(i)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exercise_interfaces = []\n",
    "index = 0\n",
    "for ex in exercises:\n",
    "    # Get the Greek sentence\n",
    "    greek_answer = exercises[index][\"greek answer\"]\n",
    "\n",
    "    # Get the English sentence\n",
    "    english_answer = exercises[index][\"english answer\"]\n",
    "\n",
    "    \n",
    "    desc = \"Translate the following sentence into Greek: \" \n",
    "    \n",
    "    exercise_interfaces.append(gr.Interface(fn=get_feedback, description=desc,\n",
    "                    inputs=[gr.Textbox(lines=1, value=greek_answer, visible=False), gr.Textbox(lines=2, placeholder=\"Enter Greek translation here...\", label=english_answer)],\n",
    "                    outputs=\"text\"))\n",
    "    index += 1\n",
    "\n",
    "# name each of the exercise tabs\n",
    "tab_names = list(range(len(exercises)))\n",
    "tab_names = [('Ex.'+str(x+1)) for x in tab_names]\n",
    "\n",
    "# Launch the interface\n",
    "user_interface = gr.TabbedInterface(exercise_interfaces, tab_names)\n",
    "user_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
