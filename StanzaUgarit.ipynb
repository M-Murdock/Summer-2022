{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes as input a comma-separated file with the format: `line number, question part, question, answer`. Each line represents a separate sentence. The first line must be: `line, part, greek, english`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyconll in /opt/anaconda3/lib/python3.8/site-packages (3.1.0)\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6ed9255dce441488ef08ca598ece03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 17:18:17 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 17:18:19 INFO: File exists: /Users/mallard/stanza_resources/en/default.zip\n",
      "2022-07-25 17:18:24 INFO: Finished downloading models and saved to /Users/mallard/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97314e58f1254f839f24f64ed92471c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 17:18:24 INFO: Downloading default packages for language: grc (Ancient_Greek)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 17:18:24 INFO: File exists: /Users/mallard/stanza_resources/grc/default.zip\n",
      "2022-07-25 17:18:25 INFO: Finished downloading models and saved to /Users/mallard/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45b203a4f7f4ca599150c45acf834ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 17:18:26 INFO: Loading these models for language: grc (Ancient_Greek):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | proiel  |\n",
      "| pos       | proiel  |\n",
      "| lemma     | proiel  |\n",
      "| depparse  | proiel  |\n",
      "=======================\n",
      "\n",
      "2022-07-25 17:18:26 INFO: Use device: cpu\n",
      "2022-07-25 17:18:26 INFO: Loading: tokenize\n",
      "2022-07-25 17:18:26 INFO: Loading: pos\n",
      "2022-07-25 17:18:26 INFO: Loading: lemma\n",
      "2022-07-25 17:18:26 INFO: Loading: depparse\n",
      "2022-07-25 17:18:26 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8589928d162c4d57bb7a1b38cf0b584f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 17:18:28 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-07-25 17:18:28 INFO: Use device: cpu\n",
      "2022-07-25 17:18:28 INFO: Loading: tokenize\n",
      "2022-07-25 17:18:28 INFO: Loading: pos\n",
      "2022-07-25 17:18:28 INFO: Loading: lemma\n",
      "2022-07-25 17:18:28 INFO: Loading: depparse\n",
      "2022-07-25 17:18:29 INFO: Loading: sentiment\n",
      "2022-07-25 17:18:29 INFO: Loading: constituency\n",
      "2022-07-25 17:18:30 INFO: Loading: ner\n",
      "2022-07-25 17:18:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "\n",
    "from xml.dom import minidom\n",
    "import os \n",
    "\n",
    "import xml.etree.ElementTree as gfg \n",
    "\n",
    "import lxml\n",
    "\n",
    "!pip install pyconll\n",
    "import pyconll\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "stanza.download('grc')\n",
    "\n",
    "nlp_grk = stanza.Pipeline('grc') \n",
    "nlp_en = stanza.Pipeline('en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 1: Store contents of csv in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isValid(myList):\n",
    "#     if myList[0][2] == '':\n",
    "#         print(\"empty space found!\")\n",
    "#         return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract only the translation exercises (i.e. for each exercise in a given csv file, it is a translation exercise if it appears in crosby-schaeffer-translation-exercises.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_translation_exs = []\n",
    "\n",
    "with open(\"crosby-schaeffer-translation-exercises-no-nums.csv\",\"r\") as my_csv:\n",
    "    all_translation_exs = my_csv.readlines()\n",
    "#     print(all_translation_exs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τοὺς πολεμίους παύουσιν\n",
      "τὸν ἄνθρωπον πέμπει ἀπὸ τοῦ Ἑλλησπόντου\n",
      "τοὺς φίλους πέμπουσιν ἐκ τοῦ ποταμοῦ\n",
      "ὁ στρατηγὸς τοὺς ἀνθρώπους πέμπει τῷ ἀδελφῷ\n",
      "στάδιον ἦν παρὰ τῷ ποταμῷ\n",
      "καὶ εἰς τὸ στάδιον ἄγουσι δῶρα καλά\n",
      "ἐκ τοῦ πεδίου ἄγομεν τοὺς πολεμίους\n"
     ]
    }
   ],
   "source": [
    "# define the file names here:\n",
    "file_list = glob.glob('lib/Crosby-Schaeffer-Lessons/*.csv')\n",
    "\n",
    "file_lines = []\n",
    "ex_index = -1\n",
    "\n",
    "# loop through each answer key file(lesson)\n",
    "for x in file_list:\n",
    "    # read the contents, save them as list\n",
    "    with open(x, newline='') as f:\n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # list of lists (list of exercises containing number, part, greek, translation)\n",
    "        l = list(reader)\n",
    "\n",
    "        for ex in l:\n",
    "            file_lines.append([])\n",
    "            ex_index = ex_index + 1\n",
    "            \n",
    "            # make sure we're just getting the exercises, not the dividers\n",
    "            if not ex[0] == '':\n",
    "                # loop through each exercise in our master list of Crosby-Schaeffer translation exercises\n",
    "                for contents in all_translation_exs:\n",
    "                    contents = contents.split(\",\")\n",
    "                    # check to see whether we're looking at a translation exercise. If so, print it\n",
    "                    if ex[2] in contents: \n",
    "#                         file_lines[ex_index].append(ex)\n",
    "                        print(ex[2]) # NOTE: ultimately, we want to store this in lines_df, not just print it\n",
    "\n",
    "    \n",
    "# convert the list to a dataframe\n",
    "# lines_df = pd.DataFrame(file_lines, columns=['line', 'part', 'greek', 'english'])\n",
    "\n",
    "# print(file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # define the file names here:\n",
    "# file_list = glob.glob('lib/Crosby-Schaeffer-Lessons/*.csv')\n",
    "\n",
    "# file_lines = []\n",
    "\n",
    "# # loop through each file (lesson)\n",
    "# for x in file_list:\n",
    "#     # read the contents, save them as list\n",
    "#     with open(x, newline='') as f:\n",
    "#         reader = csv.reader(f, delimiter = ',')\n",
    "#         l = list(reader)\n",
    "#         if not l[0][0] == '':\n",
    "#             file_lines = file_lines + l\n",
    "    \n",
    "# # convert the list to a dataframe\n",
    "# lines_df = pd.DataFrame(file_lines, columns=['line', 'part', 'greek', 'english'])\n",
    "\n",
    "# print(lines_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 2: Put dataframe info into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_nums = lines_df['line'].tolist()\n",
    "q_part = lines_df['part'].tolist()\n",
    "\n",
    "greek_lines = lines_df['greek'].tolist()\n",
    "english_lines = lines_df['english'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 3: Place the data on each word into a UD file format. Note that we can test this file with the CoNNL-U viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "[UD file format](https://universaldependencies.org/format.html)\n",
    "\n",
    "[CoNNL file viewer](https://universaldependencies.org/conllu_viewer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treebank_file = 'UDtreebank.txt'\n",
    "skip_lines = []\n",
    "with open(treebank_file, 'w') as f:\n",
    "      \n",
    "    # loop through each line of text\n",
    "    for index in range(0, len(english_lines)): \n",
    "\n",
    "        # get the data for each word\n",
    "        grk_doc = nlp_grk(greek_lines[index])\n",
    "\n",
    "        if len(grk_doc.sentences) > 0:\n",
    "#             if len(grk_doc.sentences[0].words.text) < 0:\n",
    "                \n",
    "            for grk_token in grk_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(grk_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(grk_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(grk_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(grk_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(grk_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(grk_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(grk_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(grk_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(grk_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "        \n",
    "        \n",
    "        # get the data for each word\n",
    "        en_doc = nlp_en(english_lines[index])\n",
    "\n",
    "        if len(en_doc.sentences) > 0:\n",
    "            for en_token in en_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(en_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(en_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(en_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(en_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(en_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(en_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(en_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(en_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(en_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "# Stanza Features (https://stanfordnlp.github.io/stanza/data_objects.html):\n",
    "\n",
    "# \"id\"\n",
    "# \"text\"\n",
    "# \"lemma\"\n",
    "# \"upos\"\n",
    "# \"xpos\"\n",
    "# \"feats\"\n",
    "# \"head\"\n",
    "# \"deprel\"\n",
    "# \"start_char\"\n",
    "# \"end_char\"\n",
    "# \"ner\"\n",
    "\n",
    "\n",
    "# UD Required Features:\n",
    "\n",
    "# D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "# FORM: Word form or punctuation symbol.\n",
    "# LEMMA: Lemma or stem of word form.\n",
    "# UPOS: Universal part-of-speech tag.\n",
    "# XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "# FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "# HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "# DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "# DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "# MISC: Any other annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 4: same as above, but make 2 files (one english, one greek) so we can compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_file_en = 'english_UDtreebank.txt'\n",
    "treebank_file_grk = 'greek_UDtreebank.txt'\n",
    "skip_lines = []\n",
    "with open(treebank_file_en, 'w') as f:\n",
    "      \n",
    "    # loop through each line of text\n",
    "    for index in range(0, len(english_lines)): \n",
    "  \n",
    "        # get the data for each word\n",
    "        en_doc = nlp_en(english_lines[index])\n",
    "\n",
    "        if len(en_doc.sentences) > 0:\n",
    "            for en_token in en_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(en_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(en_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(en_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(en_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(en_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(en_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(en_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(en_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(en_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "            \n",
    "            \n",
    "with open(treebank_file_grk, 'w') as f:\n",
    "      \n",
    "    # loop through each line of text\n",
    "    for index in range(0, len(english_lines)): \n",
    "\n",
    "        # get the data for each word\n",
    "        grk_doc = nlp_grk(greek_lines[index])\n",
    "\n",
    "        if len(grk_doc.sentences) > 0:\n",
    "                \n",
    "            for grk_token in grk_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(grk_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(grk_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(grk_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(grk_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(grk_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(grk_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(grk_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(grk_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(grk_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
