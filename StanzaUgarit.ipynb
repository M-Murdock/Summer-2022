{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes as input a comma-separated file with the format: `line number, question part, question, answer`. Each line represents a separate sentence. The first line must be: `line, part, greek, english`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyconll in /opt/anaconda3/lib/python3.8/site-packages (3.1.0)\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02604d4e6cac404189e797ccfc40dee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:37:30 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:37:31 INFO: File exists: /Users/mallard/stanza_resources/en/default.zip\n",
      "2022-07-18 14:37:41 INFO: Finished downloading models and saved to /Users/mallard/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd7c72629064bc3ae2c82d2cb5c14b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:37:41 INFO: Downloading default packages for language: grc (Ancient_Greek)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:37:41 INFO: File exists: /Users/mallard/stanza_resources/grc/default.zip\n",
      "2022-07-18 14:37:43 INFO: Finished downloading models and saved to /Users/mallard/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04526e651dd64f29b8333d237fd871e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:37:43 INFO: Loading these models for language: grc (Ancient_Greek):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | proiel  |\n",
      "| pos       | proiel  |\n",
      "| lemma     | proiel  |\n",
      "| depparse  | proiel  |\n",
      "=======================\n",
      "\n",
      "2022-07-18 14:37:43 INFO: Use device: cpu\n",
      "2022-07-18 14:37:43 INFO: Loading: tokenize\n",
      "2022-07-18 14:37:43 INFO: Loading: pos\n",
      "2022-07-18 14:37:43 INFO: Loading: lemma\n",
      "2022-07-18 14:37:43 INFO: Loading: depparse\n",
      "2022-07-18 14:37:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac5358c17c54cb38e788f56891d894e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 14:37:45 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-07-18 14:37:45 INFO: Use device: cpu\n",
      "2022-07-18 14:37:45 INFO: Loading: tokenize\n",
      "2022-07-18 14:37:45 INFO: Loading: pos\n",
      "2022-07-18 14:37:45 INFO: Loading: lemma\n",
      "2022-07-18 14:37:45 INFO: Loading: depparse\n",
      "2022-07-18 14:37:46 INFO: Loading: sentiment\n",
      "2022-07-18 14:37:46 INFO: Loading: constituency\n",
      "2022-07-18 14:37:47 INFO: Loading: ner\n",
      "2022-07-18 14:37:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "\n",
    "from xml.dom import minidom\n",
    "import os \n",
    "\n",
    "import xml.etree.ElementTree as gfg \n",
    "\n",
    "import lxml\n",
    "\n",
    "!pip install pyconll\n",
    "import pyconll\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "stanza.download('grc')\n",
    "\n",
    "nlp_grk = stanza.Pipeline('grc') \n",
    "nlp_en = stanza.Pipeline('en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 1: Store contents of csv in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isValid(myList):\n",
    "#     if myList[0][2] == '':\n",
    "#         print(\"empty space found!\")\n",
    "#         return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   line part                               greek  \\\n",
      "0     7    b                        ἦν στρατηγός   \n",
      "1     7    b                       ἔχει ἀδελφούς   \n",
      "2     7    b               παύουσι τὸν στρατηγόν   \n",
      "3     7    b           οἱ στρατηγοὶ ἦσαν ἀδελφοί   \n",
      "4     7    b                          ἦν ποταμός   \n",
      "5     7    b    τῷ στρατηγῷ πέμπει τοὺς ἀδελφούς   \n",
      "6     7    b               παύει τοὺς στρατηγούς   \n",
      "7     7    b  πέμπουσι τὸν τοῦ στρατηγοῦ ἀδελφόν   \n",
      "8     7    b                        ἦν στρατηγός   \n",
      "9     7    b                       ἔχει ἀδελφούς   \n",
      "10    7    b               παύουσι τὸν στρατηγόν   \n",
      "11    7    b           οἱ στρατηγοὶ ἦσαν ἀδελφοί   \n",
      "12    7    b                          ἦν ποταμός   \n",
      "13    7    b    τῷ στρατηγῷ πέμπει τοὺς ἀδελφούς   \n",
      "14    7    b               παύει τοὺς στρατηγούς   \n",
      "15    7    b  πέμπουσι τὸν τοῦ στρατηγοῦ ἀδελφόν   \n",
      "\n",
      "                                 english  \n",
      "0                       He was a general  \n",
      "1                        He has brothers  \n",
      "2                  They stop the general  \n",
      "3             The generals were brothers  \n",
      "4                     There was a river.  \n",
      "5   He sends the brothers to the general  \n",
      "6                  He stops the generals  \n",
      "7   They send the brother of the general  \n",
      "8                       He was a general  \n",
      "9                        He has brothers  \n",
      "10                 They stop the general  \n",
      "11            The generals were brothers  \n",
      "12                    There was a river.  \n",
      "13  He sends the brothers to the general  \n",
      "14                 He stops the generals  \n",
      "15  They send the brother of the general  \n"
     ]
    }
   ],
   "source": [
    "# define the file names here:\n",
    "file_list = glob.glob('lib/Crosby-Shaeffer-Lessons/*.csv')\n",
    "\n",
    "file_lines = []\n",
    "\n",
    "# loop through each file (lesson)\n",
    "for x in file_list:\n",
    "    # read the contents, save them as list\n",
    "    with open(file_list[0], newline='') as f:\n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        l = list(reader)\n",
    "        if not l[0][0] == '':\n",
    "            file_lines = file_lines + l\n",
    "    \n",
    "# convert the list to a dataframe\n",
    "lines_df = pd.DataFrame(file_lines, columns=['line', 'part', 'greek', 'english'])\n",
    "\n",
    "print(lines_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 2: Put dataframe info into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_nums = lines_df['line'].tolist()\n",
    "q_part = lines_df['part'].tolist()\n",
    "\n",
    "greek_lines = lines_df['greek'].tolist()\n",
    "english_lines = lines_df['english'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 3: Place the data on each word into a UD file format. Note that we can test this file with the CoNNL-U viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "[UD file format](https://universaldependencies.org/format.html)\n",
    "\n",
    "[CoNNL file viewer](https://universaldependencies.org/conllu_viewer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treebank_file = 'UDtreebank.txt'\n",
    "skip_lines = []\n",
    "with open(treebank_file, 'w') as f:\n",
    "      \n",
    "    # loop through each line of text\n",
    "    for index in range(0, len(english_lines)): \n",
    "\n",
    "        # get the data for each word\n",
    "        grk_doc = nlp_grk(greek_lines[index])\n",
    "\n",
    "        if len(grk_doc.sentences) > 0:\n",
    "#             if len(grk_doc.sentences[0].words.text) < 0:\n",
    "                \n",
    "            for grk_token in grk_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(grk_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(grk_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(grk_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(grk_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(grk_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(grk_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(grk_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(grk_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(grk_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "        \n",
    "        \n",
    "        # get the data for each word\n",
    "        en_doc = nlp_en(english_lines[index])\n",
    "\n",
    "        if len(en_doc.sentences) > 0:\n",
    "            for en_token in en_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(en_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(en_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(en_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(en_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(en_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(en_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(en_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(en_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(en_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "# Stanza Features (https://stanfordnlp.github.io/stanza/data_objects.html):\n",
    "\n",
    "# \"id\"\n",
    "# \"text\"\n",
    "# \"lemma\"\n",
    "# \"upos\"\n",
    "# \"xpos\"\n",
    "# \"feats\"\n",
    "# \"head\"\n",
    "# \"deprel\"\n",
    "# \"start_char\"\n",
    "# \"end_char\"\n",
    "# \"ner\"\n",
    "\n",
    "\n",
    "# UD Required Features:\n",
    "\n",
    "# D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "# FORM: Word form or punctuation symbol.\n",
    "# LEMMA: Lemma or stem of word form.\n",
    "# UPOS: Universal part-of-speech tag.\n",
    "# XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "# FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "# HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "# DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "# DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "# MISC: Any other annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 4: same as above, but make 2 files (one english, one greek) so we can compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_file_en = 'english_UDtreebank.txt'\n",
    "treebank_file_grk = 'greek_UDtreebank.txt'\n",
    "skip_lines = []\n",
    "with open(treebank_file_en, 'w') as f:\n",
    "      \n",
    "    # loop through each line of text\n",
    "    for index in range(0, len(english_lines)): \n",
    "  \n",
    "        # get the data for each word\n",
    "        en_doc = nlp_en(english_lines[index])\n",
    "\n",
    "        if len(en_doc.sentences) > 0:\n",
    "            for en_token in en_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(en_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(en_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(en_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(en_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(en_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(en_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(en_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(en_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(en_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "            \n",
    "            \n",
    "with open(treebank_file_grk, 'w') as f:\n",
    "      \n",
    "    # loop through each line of text\n",
    "    for index in range(0, len(english_lines)): \n",
    "\n",
    "        # get the data for each word\n",
    "        grk_doc = nlp_grk(greek_lines[index])\n",
    "\n",
    "        if len(grk_doc.sentences) > 0:\n",
    "                \n",
    "            for grk_token in grk_doc.sentences[0].words:\n",
    "                # D: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "                f.write(str(grk_token.id))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FORM: Word form or punctuation symbol.\n",
    "                f.write(grk_token.text)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # LEMMA: Lemma or stem of word form.\n",
    "                f.write(grk_token.lemma)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # UPOS: Universal part-of-speech tag.\n",
    "                f.write(grk_token.upos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "                f.write(grk_token.xpos)\n",
    "                f.write('\\t')\n",
    "\n",
    "                # FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "                f.write(str(grk_token.feats))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "                f.write(str(grk_token.head))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "                f.write(str(grk_token.deprel))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "                f.write(str(grk_token.deps))\n",
    "                f.write('\\t')\n",
    "\n",
    "                # MISC: Any other annotation.\n",
    "                # Sentence number/part\n",
    "                f.write(str(line_nums[index]) + q_part[index])\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
