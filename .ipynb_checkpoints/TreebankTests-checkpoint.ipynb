{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for working with [treebank data](https://github.com/gregorycrane/gAGDT/tree/master/data/xml). (The schema is described [here](https://github.com/gregorycrane/gAGDT))\n",
    "\n",
    "We will use [Stanza](https://stanfordnlp.github.io/stanza/index.html) to train a dependency parser on our data which we can use for creating exercises (like finding the subject or object of verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    TODO:\n",
    "- [ ] Use Stanza to [train](https://stanfordnlp.github.io/stanza/training.html) a dependency parser on our treebank data\n",
    "- [ ] Use our parser to create exercises (e.g. finding the subject or object of verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install stanza\n",
    "\n",
    "# Import the package\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Format all the treebank data correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Formatting\n",
    "\n",
    "- Stanza likes its annotations to be stored in  [CoNLL-U](https://universaldependencies.org/format.html) format. More info on UD guidelines can be found [here](https://universaldependencies.org/guidelines.html).\n",
    "\n",
    "- According to the note [here](https://github.com/stanfordnlp/stanza#batching-to-maximize-pipeline-speed), it increases performance if we concatenate documents together and divide them with `\\n\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to convert treebank data goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train our Parser\n",
    "\n",
    "The tutorial [here](https://github.com/stanfordnlp/stanza-train) outlines these steps:\n",
    "\n",
    "1. create a folder for holding training data, cd into that folder\n",
    "2. ```git clone git@github.com:stanfordnlp/stanza.git\n",
    "cp config/config.sh stanza/scripts/config.sh\n",
    "cp config/xpos_vocab_factory.py stanza/stanza/models/pos/xpos_vocab_factory.py\n",
    "cd stanza\n",
    "source scripts/config.sh```\n",
    "3. modify the `config.sh` script (which is used to set environment variables)\n",
    "\n",
    "NOTE: stanzatrain/config/xpos_vocab_factory.py is designed to use the UD_English-TEST and should be modified to use a different treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Use our Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a Processor\n",
    "By default, this includes all processors (tokenization, multi-word expansion, part-of-speech tagging, lemmatization, dependency parsing and named entity recognition). This can be changed with the `processors` argument\n",
    "\n",
    "Info on dependency parsing can be found [here](https://stanfordnlp.github.io/stanza/depparse.html) \n",
    "\n",
    "According to the [tutorial](https://github.com/stanfordnlp/stanza-train#initializing-processors-with-trained-models), we just need to provide the path for our model file\n",
    "\n",
    "NOTE: the code below is only a placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_model_path='saved_models/tokenize/en_test_tokenizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build an English pipeline, with all processors by default\n",
    "print(\"Building an English pipeline...\")\n",
    "en_nlp = stanza.Pipeline('en')\n",
    "\n",
    "# Build a Chinese pipeline, with customized processor list and no logging, and force it to use CPU\n",
    "print(\"Building a Chinese pipeline...\")\n",
    "zh_nlp = stanza.Pipeline('zh', processors='tokenize,lemma,pos,depparse', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate Text \n",
    "\n",
    "# Processing English text\n",
    "en_doc = en_nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "print(type(en_doc))\n",
    "\n",
    "# Processing Chinese text\n",
    "zh_doc = zh_nlp(\"达沃斯世界经济论坛是每年全球政商界领袖聚在一起的年度盛事。\")\n",
    "print(type(zh_doc))\n",
    "\n",
    "# Print information about a word\n",
    "word = en_doc.sentences[0].words[0]\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These [quizzes](https://github.com/gregorycrane/Homerica/tree/master/quizzes) contain every morphological paradigm in Iliad 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
